<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://daffupman.github.io</id>
    <title></title>
    <updated>2021-10-22T12:39:07.368Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://daffupman.github.io"/>
    <link rel="self" href="https://daffupman.github.io/atom.xml"/>
    <logo>https://daffupman.github.io/images/avatar.png</logo>
    <icon>https://daffupman.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, </rights>
    <entry>
        <title type="html"><![CDATA[Spring源码纪事本末]]></title>
        <id>https://daffupman.github.io/IOmq5uQt2/</id>
        <link href="https://daffupman.github.io/IOmq5uQt2/">
        </link>
        <updated>2021-10-22T05:57:43.000Z</updated>
        <content type="html"><![CDATA[<h2 id="一-对待spring的态度">一、对待Spring的态度</h2>
<p>我相信绝大部分Java开发人员对Spring都是很耳熟能详的，在Spring框架的帮助下，开发人员的工作会非常的快乐，但对Spring内部的认识却不尽相同。这篇博客是自己在学习Spring源码的一些记录和认识，目的有：</p>
<ol>
<li>以Spring内部的角度去看待Spring提供的各种功能，即Spring是如何提供这些能力的；</li>
<li>为了更好地去学习其他开源代码做铺垫；</li>
<li>从设计角度去看待代码如何组织起来的，即学习如何编写高质量代码。</li>
</ol>
<p>Spring代码非常复杂和庞大，需要有耐心和恒心，慢慢地啃下来，对自己的代码能力一定会有长足进步的。</p>
<h2 id="二-spring启动的流程">二、Spring启动的流程</h2>
<p>真实项目中，在启动Spring项目之前，我们一般会先提供一份Spring的配置文件。Spring会根据这个配置文件去指定的地方读取BeanDefinition，或者直接从配置文件中读取BeanDefinition，放在内部的Bean容器中。在Spring的启动过程中或启动完成，都是可以去使用这些Bean。所以SpringIOC容器是一个非常重要的角色，实际上整个过程非常的复杂，研究他对BeanDefinition的加载和使用就是我观察的重点。</p>
<p>以xml配置文件为例，配置文件中定义了一个BeanDefinition，在创建容器的时候，会把配置文件的路径告诉Spring。IOC容器根据路径，去找到这个配置文件，然后读取这个配置问文件生成Resource。接着去读取文件中的信息，获取一个个BeanDefinition，注册到BeanFactory中。这是一个大致的流程。</p>
<ul>
<li>FileSystemXmlApplicationContext(String[] configLocations, boolean refresh, @Nullable ApplicationContext parent)
<ul>
<li>设置父容器
<ul>
<li>创建resourcePatternResolver</li>
<li>设置父容器，合并父容器的Environment</li>
</ul>
</li>
<li>解析配置文件路径（将路径解析成一个个具体的路径）</li>
<li>刷新容器
<ul>
<li>prepareRefresh
<ul>
<li>设置容器的一些标志位；</li>
<li>initPropertySources：钩子函数，在AbstractRefreshableWebApplicationContext中会将ServletContextPropertySource和ServletConfigPropertySource加入到Environment的PropertySource集合中；</li>
<li>验证Environment的必要属性；</li>
<li>设置applicationListeners和applicationEvents；</li>
</ul>
</li>
<li>obtainFreshBeanFactory
<ul>
<li>refreshBeanFactory：钩子函数。AbstractRefreshableApplicationContext会重写创建beanFactory并加载beanDefinition，而GenericApplicationContext只是重新设置id；</li>
<li>getBeanFactory：钩子函数。获取beanFactory；</li>
</ul>
</li>
<li>prepareBeanFactory
<ul>
<li>给beanfactory设置beanClassLoader、beanExpressionResolver、propertyEditorRegistrar、ApplicationContextAwareProcessor、ignoredDependencyInterfaces，beanPostProcessor，</li>
<li>添加bean实例（Enviroment、SystemProperties和SystemEnvironment）</li>
</ul>
</li>
<li>postProcessBeanFactory：钩子函数，允许子类继续向beanFactory中添加设置。比如AbstractRefreshableApplicationContext会添加ServletContextAwareProcessor，ignoreDependencyInterfaces，scopes和EnvironmentBeans（ServletContext，ServletConfig）等；</li>
<li>invokeBeanFactoryPostProcessors
<ul>
<li>给beanFactory执行BeanFactoryPostProcessors。BeanFactory是否是BeanDefinitionRegistry
<ul>
<li>是： 先执行ApplicationContext和BeanFactory中所有的BeanDefinitionRegistryPostProcessor#postProcessBeanDefinitionRegistry，然后按实现了以下接口的BeanDefinitionRegistryPostProcessor去执行postProcessBeanDefinitionRegistry
<ul>
<li>PriorityOrdered</li>
<li>Ordered</li>
<li>其他的<br>
接着执行所有的BeanFactoryPostProcessors的postProcessBeanFactory方法（先执行实现了BeanDefinitionRegistryPostProcessor的BeanFactoryPostProcessor）</li>
</ul>
</li>
<li>否：执行ApplicationContext中所有的BeanFactoryPostProcessor的postProcessBeanFactory方法</li>
</ul>
</li>
<li>执行BeanFactory中所有的BeanFactoryPostProcessor的postProcessBeanFactory方法（按顺序，分实现了PriorityOrdered接口、Ordered接口和其他）</li>
<li>清除缓存</li>
<li>添加LoadTimeWeaverAwareProcessor</li>
</ul>
</li>
<li>registerBeanPostProcessors
<ul>
<li>读取beanFactory中所有的BeanPostProcessor，分实现类加入到beanFactory中</li>
<li>添加ApplicationListenerDetector</li>
</ul>
</li>
<li>initMessageSource
<ul>
<li>设置messageSource</li>
</ul>
</li>
<li>initApplicationEventMulticaster
<ul>
<li>设置applicationEventMulticaster</li>
</ul>
</li>
<li>onRefresh
<ul>
<li>钩子函数。给子类继续对ApplicationContext初始化操作</li>
</ul>
</li>
<li>registerListeners
<ul>
<li>给applicationEventMulticaster注册监听器，并发布earlyApplicationEvents</li>
</ul>
</li>
<li>finishBeanFactoryInitialization
<ul>
<li>beanFactory初始化结束后的操作
<ul>
<li>设置ConversionService</li>
<li>添加embeddedValueResolvers</li>
<li>初始化实现LoadTimeWeaverAware的Bean</li>
<li>实例化所有的BeanDefinition</li>
</ul>
</li>
</ul>
</li>
<li>finishRefresh
<ul>
<li>清除缓存，发布事件等操作</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[代码优雅之道]]></title>
        <id>https://daffupman.github.io/u-JPrYlkM/</id>
        <link href="https://daffupman.github.io/u-JPrYlkM/">
        </link>
        <updated>2021-08-15T14:25:16.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>从毕业开始以来，自己也当两年的程序员。两年的开发，实际上大多数都是CRUD操作和少部分的非业务开发。CRUD上驾轻就熟，以至于对程序产生一点点自负感：代码也就如此，自己可以去学习各种各样的中间件，甚至是换个语言。但是仔细想了想，之前在阅读spring和shiro的源码中，可是花了很多时间，照着网上的阅读思路，也就勉强懂了点皮毛。也渐渐意识到，自己的代码能力还远远不够的，但也不知如何提高。</p>
<p>直到今年年初换了一家新公司，再熟悉原来的业务代码逻辑的时候，发现了项目中的代码并非是简单的CRUD过程，而是运用了大量的设计模式。公司也有着一套封装好了的业务框架，框架源码也是运用了各种设计模式，整个代码看起来很优美。这个时候我意思到，自己写的代码是多么的丑陋不堪——用面向对象语言写着面向过程的代码，代码读起来很生硬，存在重复代码，也无扩展可言。</p>
<p>从此，开始阅读设计模式相关的资料，其中让我大有受益的是极客时间的一个专栏《设计模式之美》。文章读起来很顺畅，分析痛彻，让我看到了自己的很多问题。同时也启发着我该如何写代码。下面是我学习到的一些内容概括和总结。</p>
<h2 id="思路">思路</h2>
<p>与很多书和资料不同的是，《设计模式之美》一开始用了大量的笔墨描述了面向对象、设计原则、代码重构和编码规范，然后才是说到各种设计模式。实际上，所有的内容都在阐述一个问题。即，如何解耦代码，使得代码可读、可复用、可拓展。下面就是这个专栏的一些简要摘录和笔记。</p>
<h2 id="面向对象">面向对象</h2>
<p>在学校里学习编程的时候，就学过什么是面向对象，面向对象的特性。直到现在，才对面向对象有着一点具体的认识。再结合这部分内容来看，大概的认识有。</p>
<p>面向对象编程是一种编程范式，也就是一种编程风格。与面向过程相比较起来看，面向对象的代码组织是以类或对象组织的，并有着四大特性（或者三大特性）。我们知道，面向过程编程重在过程（函数），站着执行的角度，实际上不同的人来看这样的代码是需要照着执行逻辑去厘清思路的。而面向对象编程，是以人的思维来组织代码的，这样的代码更加可读，维护更加容易。因此面向对象更加高级。我们应有意识地去运用面向对象来改善我们的代码。在开发中，还有一点需要特别注意的是，在有需求的时候，第一步不是直接写代码，而是先做面向对象分析和面向对象设计。面向对象分析就是要搞清楚要做什么，面向对象设计就是搞清楚怎么做，最后才是面向对象编程，输出代码。</p>
<p>关于四大特性：封装、抽象、继承和多态的作用。封装的目的在于对数据做访问保护，抽象对方法的具体实现做隐藏。而继承侧重表示is-a的关系，复用代码，多态表示has-a关系，有利于代码的复用和拓展。正是由于这些丰富的特性，使得面向对象对象语言更加适合流程复杂的系统，使得项目代码易拓展、易复用和易维护。<br>
在项目中，也有着经典的面向过程的编码表现，比如：滥用getter和setter方法；Constants类和Utils类的设计问题；以及基于贫血模型的开发模式。</p>
<p>在面向对象编程中，接口和抽象类是实现代码可复用、可维护和可拓展的重要的基础的工具。接口和抽象类有着相似的地方，不同点也同样明显。在有的语言中，接口是可以替代抽象类的。“基于接口而非实现编程”这条原则不仅仅教我们该怎么写代码，也指导来架构的设计和系统的设计。最后，我们还需要知道“组合优于继承”的原因，该如何使用组合和使用继承。</p>
<h2 id="设计原则">设计原则</h2>
<p>正是因为面向对象突出的特点，很多语言也支持面向对象。在软件开发中，有很多的设计原则都是基于面向对象的。包括：SOLID、KISS、YAGNI、DRY和LOD等。</p>
<h3 id="单一职责原则srp">单一职责原则（SRP）</h3>
<p>单一职责原则要求一个类或模块只负责完成一个职责（或功能）。不要设计大而全的类，要设计粒度小、功能单一的类。单一职责是为了实现代码高内聚、低耦合，提高代码的复用行、可读性和可维护性。代码是否职责单一的评判有这样的指导标准：</p>
<ul>
<li>类中的代码行数、函数或属性过多，影响代码的可读和可维护；</li>
<li>类依赖的其他类过多，或者依赖类的其他类过多，不符合高内聚、低耦合的设计思想；</li>
<li>私有方法过多；</li>
<li>比较难给类起一个合适的名字。</li>
</ul>
<p>同时，类职责单一，类依赖的和被依赖的其他类也变少，减少代码的耦合性，以此来实现代码的高内聚、低耦合。但是如果拆分的过膝，实际上会适得其反，反倒会降低内聚性和可维护性。</p>
<h3 id="开闭原则ocp">开闭原则（OCP）</h3>
<p>软件实体应该对拓展开放、对修改关闭。对于新增一个功能，开闭原则不是说完全杜绝修改，而是以最小的修改代价。实际开发中，我们需要时刻具备扩展意识，抽象意识和封装意识。在写代码的适合，需要考虑如何设计代码结构，事先留好扩展点，以便将来需求变更的时候可以在不改动代码或最小修改来完成新功能。</p>
<h3 id="里氏替换原则lsp">里氏替换原则（LSP）</h3>
<p>子类对象能够替换程序中父类对象出现的任何地方，并且保证原来程序的逻辑行为不变即正确性不被破坏。里氏替换原则用来指导继承关系中子类该如何设计的一个原则。父类定义了函数的约定（协议），那子类可以改变函数的内部实现逻辑，但不能改变函数原有的约定。这里的约定包含：函数声明要实现的功能；对输入、输出、异常的约定，甚至包括注视中所罗列的任何特殊说明。</p>
<p>LSP与多态有相似之处，但它们关注的角度不一样。多态是面向对象编程的一大特性，也是面向对象编程语言的一种语法，他是一种代码实现思路。而里氏替换原则是一种设计原则。</p>
<h3 id="接口隔离原则isp">接口隔离原则（ISP）</h3>
<p>客户端不应该强迫依赖它不需要的接口。对于“接口”的理解，可以有：</p>
<ol>
<li>一组接口集合。可以是某个微服务的接口，也可以是某个类库的接口。如果部分接口只被部分调用者使用，那么就需要将这部分接口隔离出来，单独给这部分调用者使用，而不是强迫其他调用者也依赖这部分不会被用到的接口。</li>
<li>单个API接口或函数，部分调用只需要函数中的部分功能，那么就需要把函数分成粒度更细的多个函数，让调用者只依赖它需要的函数。</li>
<li>OOP中的接口。即接口设计要尽量单一，不要让接口的实现类和调用者，依赖不需要的接口函数。</li>
</ol>
<p>SRP针对的是模块、类和接口的设计。ISP相对于SRP，一方面更侧重接口的设计，另一方面他的思考角度也是不同的。ISP提供了一种接口的职责是否单一的判断标准：通过调用者如何使用接口来间接地判断。如果调用者只使用部分接口或接口的部分功能，那接口的设计就不够职责单一。</p>
<h3 id="依赖反转原则dip">依赖反转原则（DIP）</h3>
<p>高层模块不要依赖低层模块。高层模块和低层模块应该通过抽象来相互依赖。这个原则主要用来指导框架层面的设计。高层模块是调用者，低层模块是被调用的代码，两者之间应该通过一层抽象来连接，而不是直接相互关联。</p>
<p>控制反转是一种设计思想。“控制”指的是程序员对代码流程的控制，“反转”指的是程序员不再直接控制代码流程，而是将控制权交给框架。在具体操作上，依赖注入是常用的手段。</p>
<h3 id="kiss原则与yagni原则">KISS原则与YAGNI原则</h3>
<p>KISS原则要求代码尽量保持简单。而如果判断代码是简单的，并不是代码行数少，代码就简单，还要考虑代码逻辑的复杂度，实现难度和可读性等。这里有几条判断代码是否符合KISS原则的指导建议：</p>
<ol>
<li>不要使用同事可能不懂的技术来实现代码；</li>
<li>不重复造轮子，善于使用已有的工具类库；</li>
<li>不要过度优化，不要过度使用一些奇技淫巧来优化代码，牺牲代码的可读性。</li>
</ol>
<p>YAGNI：You Aint't Gonna Need It。你不会需要它。也就是说不要去设计当前用不到的功能，不要去编写当前用不到的代码。这条原则的核心思想：不要过度设计。KISS原则说的“如何做”的问题，YAGNI原则说的“要不要做”的问题。</p>
<h3 id="dry原则">DRY原则</h3>
<p>DRY原则：Don't Repeat Yourself。指的是不要写重复的代码。那如何判断代码是否是重复的。实际上，代码的重复分为：实现逻辑重复、功能语义重复和代码执行重复。 实现逻辑重复，但功能语义不重复的代码，不违反DRY原则。实现逻辑不重复，但功能语义重复的代码，也算违反DRY原则。</p>
<p>满足DRY原则的方法一般是复用代码。代码复用的方法一般有：</p>
<ul>
<li>减少代码耦合</li>
<li>满足单一职责原则</li>
<li>模块化</li>
<li>业务与非业务逻辑分离</li>
<li>通用代码下沉</li>
<li>继承、多态、抽象、封装</li>
<li>应用模版等设计模式</li>
</ul>
<p>但是不要为了不明确的、开发成本高的代码而强行复用代码，我们可以在持续重构中慢慢的将代码复用。相对于代码的可复用性，DRY原则适用性更强一些。我们可以不写可复用的代码，但一定不能写重复的代码（语义重复）。</p>
<h3 id="迪米特法则lod">迪米特法则（LOD）</h3>
<p>LOD也叫最小知识原则。每个模块之应该了解那些与它关系亲密的模块。LOD是希望减少类之间的耦合，让类越独立越好，每个类都应该少了解系统的其他部分。一旦发生变化，需要了解这一变化的类接回比较少。LOD也是实现代码高内聚、低耦合的一个原则。</p>
<h2 id="代码重构">代码重构</h2>
<p>重构和持续重构对于多数工程师来说，是个熟悉的名词，但真正做到的却很少。做到重构需要我们能洞察出代码存在的坏味道或者设计上的不足，并且能合理地、熟练地利用设计思想、原则、模式和编程规范等理论只是解决这些问题。对于重构有这样几个问题。</p>
<ol>
<li>重构的目的。对于项目而言，重构可以保持代码持续处于一个可控状态。对个人来说，可以运用各种设计思想、设计原则、设计模式和编程规范来锻炼代码能力。</li>
<li>重构的对象。分为大规模高层次和小规模低层次的重构。大规模高层次包括对代码分层、模块化、解藕、梳理类之间的交互关系、抽象复用组件等。小规模低层次即是利用编码规范来重构代码。</li>
<li>重构的时机。需要建立持续重构意识，重构是开发中必不可少的一部分。</li>
<li>重构的方法。小规模的重构可以随时进行，而大规模的需要有计划、有组织的进行。</li>
</ol>
<p>单元测试是代码层面的测试，由开发人员自行编写，用于测试代码正确性的检查。单元测试本身就是CodeReview和重构的过程，需要针对代码设计各种测试用例，还是TDD可落地执行的改进方案。对于代码可测试性，也就是说代码能不能容易的进行测试，需要一些有效手段来优化。依赖注入是重要的一个手段，其中需要特别注意反模式有：</p>
<ul>
<li>未决行为逻辑</li>
<li>滥用可变全局变量</li>
<li>滥用静态方法</li>
<li>使用复杂的继承关系</li>
<li>高度耦合的代码</li>
</ul>
<p>复杂代码往往不可读、不易维护。解藕是控制复杂代码的有效手段，保证代码质量。在发现类与类之间关系复杂，或者修改某处代码发生牵一发而动全身时就需要对代码解藕。方法就是运用设计思想、设计原则和设计模式等。</p>
<h2 id="编码规范">编码规范</h2>
<p>相对于设计思想和设计原则，编码规范是可以直接落地执行的。</p>
<h3 id="命名与注释">命名与注释</h3>
<p>命名需要在能准确表达意图的前提下，命名越短越好，可以利用上下文简化命名，命名要可读、可搜索。同时在接口和抽象类的命名上，要特别注意。注释的内容要充分，但不要多余。</p>
<h3 id="代码风格">代码风格</h3>
<p>类和函数的代码行数多少比较合适、一行代码多长比较合适，使用空行分割单元块，类中成员排列顺序等等。</p>
<h3 id="编程技巧">编程技巧</h3>
<p>在设计函数时，其参数数量不要过多，代码要分割成更小的单元块，不要用参数来控制逻辑，设计函数要符合职责单一，不要过深的嵌套层次，使用解释性变量。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vue CLI + Antd搭建前端项目]]></title>
        <id>https://daffupman.github.io/fygrZpQg6/</id>
        <link href="https://daffupman.github.io/fygrZpQg6/">
        </link>
        <updated>2021-04-19T12:30:59.000Z</updated>
        <content type="html"><![CDATA[<h2 id="安装vue-cli">安装vue cli</h2>
<ol>
<li>配置代理<pre><code class="language-bash">npm get registry
npm config set registry http://registry.npm.taobao.org
</code></pre>
</li>
<li>安装vue cli<pre><code class="language-bash">npm install -g @vue/cli@4.5.9
vue --version
</code></pre>
</li>
<li>创建vue项目<pre><code class="language-bash">cd /vue-example
vue create vue-example
# 选择manully select features
# 增加勾选Typescript、Router和Vuex
# 选择3.x
# class-style：no
# babel alongside typescript：no
# use history mode：yes
# linter：ESLint with error prevention only
# additional lint：lint on save
# where do you prefer：in dedicated config files
# save this：y
# save as：example
</code></pre>
</li>
<li>运行项目<pre><code class="language-bash">npm run serve
</code></pre>
</li>
</ol>
<h2 id="集成ant-design-vue2x之后的版本支持vue3">集成Ant Design Vue（2.x之后的版本支持vue3）</h2>
<ol>
<li>安装依赖<pre><code class="language-bash">npm install ant-design-vue@2.0.0-rc.3 --save
</code></pre>
</li>
<li>在main.ts中引入antd<pre><code class="language-typescript">import antd from 'ant-design-vue'
import 'ant-design-vue/dist/antd.css'
</code></pre>
</li>
<li>使用antd组件<pre><code class="language-typescript">.use(antd)
</code></pre>
</li>
</ol>
<h2 id="状态管理工具-vuex">状态管理工具 Vuex</h2>
<ol>
<li>
<p>什么是Vuex<br>
集中式存储管理应用的所有组件的状态。状态管理应用包含：</p>
<ul>
<li>state：驱动应用的数据源</li>
<li>view：以声明方式将state映射到视图</li>
<li>actions：响应在view上的用户输入导致的状态变化</li>
</ul>
</li>
<li>
<p>Vuex的特点</p>
<ul>
<li>状态存储是响应式的</li>
<li>不能直接改变store中的状态，唯一途径就是显式地提交mutation</li>
</ul>
</li>
<li>
<p>安装依赖</p>
<pre><code class="language-bash">npm install vuex
</code></pre>
</li>
<li>
<p>在main.ts中导入组件</p>
<pre><code class="language-typescript">import {createStore} from 'vuex'
</code></pre>
</li>
<li>
<p>使用vuex</p>
<pre><code class="language-typescript">const store = createStore({})
.use(store)
</code></pre>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[分布式文件系统FastDFS]]></title>
        <id>https://daffupman.github.io/ceDfb9-tr/</id>
        <link href="https://daffupman.github.io/ceDfb9-tr/">
        </link>
        <updated>2021-03-14T04:10:17.000Z</updated>
        <content type="html"><![CDATA[<h2 id="什么是fastdfs">什么是FastDFS</h2>
<p>FastDFS是一个开源的轻量级分布式文件系统，它可以管理文件，功能包括：文件存储、文件同步和文件访问，解决了大容量存储和负载均衡的问题。</p>
<p>FastDFS服务端的两个角色：跟踪器（tracker）和存储节点（storage）。两者可以由一台或多台服务器构成。为了支持大容量，存储节点采用分卷的组织方式。存储系统由一个或多个卷组成，卷和卷之间的文件是相互独立的。当存储空间不足时，可以动态增加卷来扩大存储系统的容量。FastDFS中的文件标识分为两个部分：卷名和文件名。</p>
<p>常见术语：</p>
<ul>
<li>tracker：追踪服务，用于协调和负载均衡，同时监控storage的状态；</li>
<li>storage：存储服务器，保存文件和文件的元数据；</li>
<li>group：组，同组节点提供冗余备份，不同组用于扩容；</li>
<li>meta data：文件的元数据。</li>
</ul>
<h2 id="fastdfs的架构">FastDFS的架构</h2>
<figure data-type="image" tabindex="1"><img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/20210314122731.png" alt="FastDFS架构" loading="lazy"></figure>
<p>客户端会向tracker发起请求上传，tracker会在storage中选择一个位置。tracker返回这个位置给客户端，客户端再向storage发起文件上传。storage存储完成后返回文件存储的相关信息。<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/20210314123140.png" alt="文件上传" loading="lazy"></p>
<h2 id="安装">安装</h2>
<ol>
<li>
<p>软件准备</p>
<ul>
<li>libfatscommon</li>
<li>FastDFS</li>
<li>fastdfs-nginx-module</li>
<li>nginx</li>
</ul>
</li>
<li>
<p>安装依赖</p>
<pre><code class="language-shell">yum -y install zlib zlib-devel pcre pcre-devel gcc gcc-c++ openssl openssl-devel libevent libevent-devel perl unzip wget
</code></pre>
</li>
<li>
<p>安装libfastcommon</p>
<pre><code class="language-shell">tar -zxvf libfastcommon-1.0.42.tar.gz -C /usr/local
cd /usr/local/libfastcommon-1.0.42/
./make.sh &amp;&amp; ./make.sh install
</code></pre>
</li>
<li>
<p>安装fastdfs程序</p>
<pre><code class="language-shell">tar -zxvf fastdfs-6.04.tar.gz -C /usr/local
cd /usr/local/fastdfs-6.04/
vi make.sh # 修改make文件
TARGET_PREFIX=$DESTDIR/usr
TARGET_CONF_PATH=$DESTDIR/etc/fdfs
TARGET_INIT_PATH=$DESTDIR/etc/init.d
./make.sh &amp;&amp; ./make.sh install
cp /usr/local/fastdfs-6.04/conf/* /etc/fdfs/
</code></pre>
</li>
<li>
<p>配置fastdfs</p>
<pre><code class="language-shell"># 配置tracker
vi tracker.conf

# 修改如下内容
base_path=/usr/local/fastdfs-6.04/tracker

# 配置storage
vi storage.conf

# 修改内容如下
# 修改组名 
group_name=daff01
# 修改storage的工作空间 
base_path=/usr/local/fastdfs-6.04/storage 
# 修改storage的存储空间 
store_path0=/usr/local/fastdfs-6.04/storage 
# 修改tracker的地址和端口号，用于心跳 
tracker_server=192.168.65.20:22122 
# 后续结合nginx的一个对外服务端口号 
http.server_port=8888
</code></pre>
</li>
<li>
<p>创建目录</p>
<pre><code class="language-shell">mkdir /usr/local/fastdfs-6.04/tracker -p
mkdir /usr/local/fastdfs-6.04/storage -p
</code></pre>
</li>
<li>
<p>启动tracker、storage</p>
<pre><code class="language-shell">/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf
/usr/bin/fdfs_storaged /etc/fdfs/storage.conf
ps -ef|grep tracker
/usr/bin/stop.sh /etc/fdfs/tracker.conf
</code></pre>
</li>
<li>
<p>fastdfs的nginx模块</p>
<pre><code class="language-shell">tar -zxvf fastdfs-nginx-module-1.22.tar.gz -C /usr/local
cp mod_fastdfs.conf /etc/fdfs
mkdir /usr/local/fastdfs-6.04/tmp
# 修改mod_fastdfs.config文件
base_path=/usr/local/fastdfs-6.04/tmp 
tracker_server=192.168.65.20:22122 
group_name=daff01
url_have_group_name = true  # url上会携带group_name
store_path0=/usr/local/fastdfs-6.04/storage

vi /usr/local/fastdfs-nginx-module-1.22/src/config # 修改config文件，将两处的/usr/local/include的路径删除local
# 重新配置nginx
./configure \
    --prefix=/usr/local/nginx-1.18.0 \
    --pid-path=/var/run/nginx/nginx.pid \
    --lock-path=/var/lock/nginx.lock \
    --error-log-path=/var/log/nginx/error.log \
    --http-log-path=/var/log/nginx/access.log \
    --with-http_gzip_static_module \
    --http-client-body-temp-path=/var/temp/nginx/client \
    --http-proxy-temp-path=/var/temp/nginx/proxy \
    --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \
    --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \
    --http-scgi-temp-path=/var/temp/nginx/scgi \
    --add-module=/usr/local/fastdfs-nginx-module-1.22/src
make &amp; make install # 编译和安装

# nginx配置文件增加访问文件路径
server {
    listen 8888;
    server_name localhost;
    location /daff01/M00 {
        ngx_fastdfs_module;
    }
}

./nginx # 启动nginx
</code></pre>
</li>
<li>
<p>测试</p>
<pre><code class="language-shell"># 配置client.conf
vi /etc/fdfs/client.conf
base_path=/usr/local/fastdfs-6.04/client
tracker_server=192.168.65.20:22122

# 使用测试图片上传
/usr/bin/fdfs_test /etc/fdfs/client.conf upload /opt/what-is-nginx.png
# 访问
http://192.168.65.20:8888/daff01/M00/00/00/wKhBFGBOUDiAdfcvAAATbWLJjlE268_big.png
</code></pre>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Git操作指南]]></title>
        <id>https://daffupman.github.io/br1NMVbE5/</id>
        <link href="https://daffupman.github.io/br1NMVbE5/">
        </link>
        <updated>2021-03-14T01:03:45.000Z</updated>
        <content type="html"><![CDATA[<h2 id="快速上手">快速上手</h2>
<ol>
<li>git下载地址：https://git-scm.com/</li>
<li>配置用户信息<pre><code class="language-shell">git config --global user.name = &lt;username&gt;   # 配置用户名，加&quot;&quot;
git config --global user.email = &lt;useremail&gt;    # 配置邮箱，加&quot;&quot;
git config --list # 查询配置信息
</code></pre>
</li>
<li>创建目录并初始化仓库，会生成一个<code>.git</code>的隐藏文件夹<pre><code class="language-shell">git init &lt;dirName&gt;
</code></pre>
</li>
<li>关联远程仓库<pre><code class="language-shell">git remote add &lt;name&gt; &lt;url&gt;		# 添加远程仓库
git remote						#列出所有远程仓库
</code></pre>
</li>
<li>复制远程仓库中的项目<pre><code class="language-shell">git clone &lt;url&gt;
</code></pre>
</li>
<li>文件提交<pre><code class="language-shell">git add &lt;file&gt;						#把文件从workspace添加进index中
git commit &lt;file&gt; -m &lt;comment&gt;		#把文件从index添加进repostory中
</code></pre>
</li>
<li>从远程仓库更新项目<pre><code class="language-shell">git pull &lt;name&gt; master
</code></pre>
</li>
<li>提交到远程仓库<pre><code class="language-shell">git push &lt;name&gt; master
</code></pre>
</li>
</ol>
<h2 id="git原理">Git原理</h2>
<h3 id="git工作空间">Git工作空间</h3>
<figure data-type="image" tabindex="1"><img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/1548725462646.png" alt="" loading="lazy"></figure>
<h3 id="git常用命令">Git常用命令</h3>
<ul>
<li>
<p>查看当前状态</p>
<pre><code class="language-shell">git status
</code></pre>
</li>
<li>
<p>把指定文件恢复到暂存区</p>
<pre><code class="language-shell">git reset HEAD &lt;file&gt;
</code></pre>
</li>
<li>
<p>把暂存区的文件恢复并覆盖到工作区</p>
<pre><code class="language-shell">git checkout -- &lt;file&gt;
</code></pre>
</li>
<li>
<p>查看提交历史</p>
<pre><code class="language-shell">git log
</code></pre>
</li>
<li>
<p>文件在各个空间的切换<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/1543225543060.png" alt="" loading="lazy"><br>
HEAD指针指向仓库中的当前版本：<code>HEAD~n</code>,表示前第n个版本；<br>
移动HEAD指向，指向上一个快照；再将指针指向的快照回滚带暂存区（默认情况）</p>
<pre><code class="language-shell">git reset --mixed HEAD~
</code></pre>
<p>移动HEAD指针的指向，指向上一个快照</p>
<pre><code class="language-shell">git reset --soft HEAD~
</code></pre>
<p>移动HEAD指针的指向，指向上一个快照；将指向的快照回滚到暂存区；将暂存区的文件还原到工作区</p>
<pre><code class="language-shell">git reset --hard HEAD~
</code></pre>
</li>
<li>
<p>版本回滚<br>
回滚到指定快照</p>
<pre><code class="language-shell">git log			#查看快照id（一串哈希值）
git reset [id]	#回到该id快照
</code></pre>
</li>
<li>
<p>版本比较</p>
<ul>
<li>比较工作空间和暂存目录<pre><code class="language-shell">git diff
</code></pre>
</li>
<li>比较两个历史快照<pre><code class="language-shell">git diff 快照id1 快照id2
</code></pre>
</li>
<li>比较当前工作去和仓库中的快照<pre><code class="language-shell">git diff 快照id
</code></pre>
</li>
<li>比较暂存区和仓库的快照(不加快照id默认是HEAD指向的版本)<pre><code class="language-shell">git diff --cached 快照ud
</code></pre>
</li>
<li>奥义图<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/1543226372637.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>其他</p>
<ul>
<li>更正最新一次的提交备注<pre><code class="language-shell">git commit --amend -m &quot;description&quot;
</code></pre>
</li>
<li>删除文件（除了删除工作区域和暂存区域的该文件外，还需要修改仓库HEAD指向，将其指向上一个版本，上个版本是不包含该文件的）<pre><code class="language-shell">git rm &lt;file&gt;
git reset --soft HEAD~
</code></pre>
</li>
<li>删除暂存区域的文件， 不会删除工作目录的文件<pre><code class="language-shell">git rm --cached &lt;file&gt;
</code></pre>
</li>
<li>重命名文件<pre><code class="language-shell">git mv &lt;oldname&gt; &lt;newname&gt;
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="git分支">Git分支</h2>
<ul>
<li>创建新分支<pre><code class="language-shell">git branch &lt;branchname&gt;
</code></pre>
</li>
<li>--decorate:下那是tag信息； --oneline:精简显示每个快照； --graph:图形化显示<pre><code class="language-shell">git log --decorate --oneline --graph -all
</code></pre>
</li>
<li>切换分支，此时HEAD指针会指向切换的分支上<pre><code class="language-shell">git checkout &lt;brachname&gt;
git checkout -b &lt;branchname&gt;		#创建分支，并切换到新分支中
</code></pre>
</li>
<li>合并分支<pre><code class="language-shell">git merge &lt;branchname&gt;
</code></pre>
</li>
<li>删除分支<pre><code class="language-shell">git branch -d &lt;branchname&gt;
</code></pre>
</li>
</ul>
<h2 id="ssh授权">ssh授权</h2>
<ol>
<li>授权命令，以生成公钥和私钥： <code>ssh-keygen -t rsa</code></li>
<li>在github上的settings-&gt;SSH and GPG keys中添加公钥</li>
<li>验证是否授权成功：<code>ssh -T git@github.com</code></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[入坑Nginx]]></title>
        <id>https://daffupman.github.io/yjClS6znS/</id>
        <link href="https://daffupman.github.io/yjClS6znS/">
        </link>
        <updated>2021-03-13T11:38:59.000Z</updated>
        <content type="html"><![CDATA[<h2 id="什么是nginx">什么是Nginx</h2>
<p>Nginx是一个高性能的HTTP和反向代理web服务器，同时也提供IMAP/POP3/SMTP服务。主要功能是反向代理，通过配置文件可以实现集群和负载均衡。也可用于静态资源虚拟化（部署静态资源）。</p>
<p>正向代理：代理客户端，客户端在请求服务时，会统一从正向代理服务器出发，去寻找目标服务器。<br>
反向代理：代理服务端，客户端的请求到达目标服务的时候，会统一进入反向服务器，由反向代理服务器决定去访问哪一台机器。</p>
<h2 id="安装nginx">安装nginx</h2>
<h3 id="下载软件">下载软件</h3>
<p>软件下载地址：http://nginx.org/en/download.html，选择稳定版本。</p>
<h3 id="安装依赖软件">安装依赖软件</h3>
<ul>
<li>gcc-c++：gcc环境</li>
<li>pcre pcre-devel：pcre库，用于解析正则表达式</li>
<li>zlib zlib-devel：安装zlib，压缩和解压缩依赖</li>
<li>openssl openssl-devel：ssl安全的加密的套接字协议层，用户HTTP安全传输，即https</li>
</ul>
<pre><code class="language-shell">yum install -y gcc-c++ pcre pcre-devel zlib zlib-devel openssl openssl-devel
</code></pre>
<h3 id="安装nginx-2">安装nginx</h3>
<pre><code class="language-shell">tar -zxvf nginx-1.18.0.tar.gz -C /usr/local # 解压
mkdir /var/temp/nginx /var/run/nginx /var/log/nginx -p # 编译前，先创建nginx临时目录，如果不创建，在启动nginx时会报错
./configure \
--prefix=/usr/local/nginx-1.18.0 \
--pid-path=/var/run/nginx/nginx.pid \
--lock-path=/var/lock/nginx.lock \
--error-log-path=/var/log/nginx/error.log \
--http-log-path=/var/log/nginx/access.log \
--with-http_gzip_static_module \
--http-client-body-temp-path=/var/temp/nginx/client \
--http-proxy-temp-path=/var/temp/nginx/proxy \
--http-fastcgi-temp-path=/var/temp/nginx/fastcgi \
--http-uwsgi-temp-path=/var/temp/nginx/uwsgi \
--http-scgi-temp-path=/var/temp/nginx/scgi \
--add-module=/usr/local/fastdfs-nginx-module-1.22/src # 配置命令
make &amp; make install # 编译和安装
./nginx # 启动nginx
</code></pre>
<blockquote>
<p>配置命令说明：</p>
<ul>
<li>--prefix：指定nginx安装目录</li>
<li>--pid-path：指定nginx的pid</li>
<li>--lock-path：锁定安装文件，防止被恶意篡改或误操作</li>
<li>--error-log：错误日志</li>
<li>--http-log-path：http日志</li>
<li>--with-http_gzip_static_module：启用gzip模块，在线实时压缩输出数据流</li>
<li>--http-client-body-temp-path：设置客户端请求的临时目录</li>
<li>--http-proxy-temp-path：设置http代理临时目录</li>
<li>--http-fastcgi-temp-path：设置fastcgi临时目录</li>
<li>--http-uwsgi-temp-path：设置uwsgi临时目录</li>
<li>--http-scgi-temp-path：设置scgi临时目录</li>
<li>--add-module：添加nginx的模块，如果没有就不用加，否则make会报错</li>
</ul>
</blockquote>
<h3 id="测试">测试</h3>
<p>访问nginx安装的服务</p>
<h3 id="常见问题">常见问题</h3>
<ul>
<li>如果nginx报了nginx.pid打开失败，重新手动创建上层目录即可;</li>
<li>如果报pid无效，使用命令./nginx -c /usr/local/conf/nginx.conf重新指定配置文件即可</li>
</ul>
<h2 id="常用命令">常用命令</h2>
<ul>
<li>启动nginx：./nginx</li>
<li>停止nginx：./nginx -s stop</li>
<li>重新加载配置文件并运行nginx：./nginx -s reload</li>
<li>配置文件的语法检查：./nginx -t</li>
<li>相比stop命令会更好的停止nginx，不再接受请求，当会处理完已经接受的所有请求：./nginx -s quit</li>
<li>查询nginx版本：./nginx -v</li>
<li>查询nginx版本、gcc版本和配置参数：./nginx -V</li>
<li>帮助文档：./nginx -h</li>
</ul>
<h2 id="nginx的内部原理">nginx的内部原理</h2>
<h3 id="进程模型">进程模型</h3>
<p>nginx启动后默认会产生两个进程：</p>
<ul>
<li>master：主进程，负责接受客户端指令，分发指令给worker进程；</li>
<li>worker：工作进程，负责处理指令；</li>
</ul>
<p>master进程会接受命令信号，如./nginx -t，然后再发给所有的工作进程去执行命令。默认只有一个主进程，一个工作进程。工作进程是可以通过参数worker_processes自由配置的，建议为cpu核心数-1。<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/Image.png" alt="nginx进程模型" loading="lazy"></p>
<h3 id="worker抢占机制">worker抢占机制</h3>
<p>master进程在初始化的时候会按照配置，fork出相应数量的worker进程。当客户端发来一个请求的时候，这些空闲的worker进程会先去争抢accept_mutex锁，获取锁的worker进程将会处理这个请求。</p>
<h3 id="事件处理">事件处理</h3>
<p>采用的是异步非阻塞的模型epoll，配置文件对应的是use epoll，这是默认项。</p>
<pre><code class="language-bash">events {
    # 可不配置，linux中默认为epoll
    use epoll;
    # 每个worker进程允许连接的最大连接数
    worker_connections 1024;
}
</code></pre>
<h2 id="nginx的配置">nginx的配置</h2>
<h3 id="示例">示例</h3>
<p>当有请求发到80端口时，localhost这个服务会监听到，对于默认的页面，会去html目录下去寻找index.html或index.htm作为首页。</p>
<pre><code class="language-bash">http {
    server {
        listen         80;
        server_name    localhost;

        location / {
            root     html;
            index    index.html index.htm;
        }
    }
}
</code></pre>
<h3 id="配置结构">配置结构</h3>
<figure data-type="image" tabindex="1"><img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/20210313212020.png" alt="nginx配置结构" loading="lazy"></figure>
<h3 id="main部分">main部分</h3>
<ul>
<li>user：设置worker进程用户，会涉及到nginx操作目录或文件的一些权限，默认为nobody；</li>
<li>worker_processes：worker进程工作数设置，一般为CPU数，或CPU数-1；</li>
<li>日志级别：debug、info、notice、warn、error、crit、alert、emerg；</li>
<li>pid：设置nginx进程pid；</li>
<li>include：引入外部配置，提高可读性，避免单个配置文件过大；</li>
<li>log_format：设定日志格式；</li>
<li>sendfile和tcp_nopush：sendfile使用高效文件传输，提升传输性能，启用后才能使用tcp_nopush，是指当数据表累积一定大小后才发送，提高了效率；</li>
<li>keepalive_timeout：设置客户端与服务端请求的超时时间，保证客户端多次请求的时候不会重复建立新的连接，节约资源损耗；</li>
<li>gzip：启用压缩，html/js/css压缩后传输会更快；</li>
<li>gzip_min_length：小于gzip_min_length个字节的文件不压缩；</li>
<li>gzip_comp_level：定义压缩比，压缩比越大，cpu消耗越多；</li>
<li>gzip_types：压缩文件的类型</li>
</ul>
<h3 id="events">events</h3>
<ul>
<li>use：使用的事件模型；</li>
<li>worker_connections：工作进程的最大连接数；</li>
</ul>
<h3 id="http部分">http部分</h3>
<ul>
<li>listen：监听的端口</li>
<li>server_name：服务主机名称</li>
<li>location：请求路由映射，匹配拦截</li>
<li>root：资源的位置</li>
<li>index：首页位置</li>
</ul>
<h2 id="nginx的三大功能">nginx的三大功能</h2>
<h3 id="静态服务器">静态服务器</h3>
<p>为静态文件配置一个server，把静态文件放在root指定的路径下即可，为保护静态文件在服务上的真实路径，可以使用alias。此时请求可以为/static/a.png，对应服务器上的文件为/home/file/a.png。</p>
<pre><code class="language-bash">location /static {
    alias /home/file
}
</code></pre>
<p>location的匹配规则：</p>
<ol>
<li>空格：普通匹配</li>
<li>=：精确匹配</li>
<li>~*：正则匹配，不区分大小写</li>
<li>~：正则匹配，区分大小写</li>
<li>^~：以某个字符开头</li>
</ol>
<h3 id="反向代理">反向代理</h3>
<p>需要给匹配到的请求配置上游服务器。示例：</p>
<pre><code class="language-bash">http {
    # 上游服务器
    upstream tomcats {
        server 192.168.35.20;
        server 192.168.35.30;
        server 192.168.35.40;
    }
    server {
        listen 80;
        server_name tomcats;
        location / {
            proxy_pass http://tomcats;
        }
    }
}
</code></pre>
<h3 id="负载均衡">负载均衡</h3>
<p>示例如反向代理，也就是示例使用的默认的策略（轮询）。轮询的算法有：</p>
<ol>
<li>
<p>轮询：默认的策略；</p>
</li>
<li>
<p>加权轮询：weight；</p>
</li>
<li>
<p>ip_hash：hash(ip) % node_counts = index，只计算ip的前三段。在使用hash算法的时候，如果要关掉某个机器，需要先标记为down，然后移除；</p>
</li>
<li>
<p>一致性hash：在新增或移除服务器的时候，仍然可以使大部分的用户访问原来的节点；<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/lb_alg_hash.png" alt="一致性哈希算法" loading="lazy"></p>
</li>
<li>
<p>url_hash：对url做哈希；</p>
<pre><code class="language-bash">upstream tomcats {
    hash $request_uri
    server 192.168.35.20;
}
</code></pre>
</li>
<li>
<p>least_conn：请求去往连接数最少的服务器。</p>
<pre><code class="language-bash">upstream tomcats {
    least_conn;
    server 192.168.35.20;
}
</code></pre>
</li>
</ol>
<h4 id="upstream的指令参数">upstream的指令参数</h4>
<ul>
<li>max_conns：限制一台服务的最大连接数，默认为0，即不做任何限制<pre><code class="language-bash">upstream tomcats {
    server 192.168.35.20 max_conns=2;
}
</code></pre>
</li>
<li>slow_start：可以设置一个时间，在服务器启动时，nginx会给这个服务器的权重在slow_start时间内，从0升到weight。商业版可用，不能使用在hash和random load balancing中，如果upstream中只有一台服务器，则该参数失效；</li>
<li>down：表示某台机器不可用；</li>
<li>backup：让某台机器成为备用机，如果集群正常，备用机不处理请求，nginx中的其他所有的机器都宕机之后，会自动启用，不能使用在hash和random load balancing中；</li>
<li>max_fails和fail_timeout<pre><code class="language-bash">upstream tomcats {
    # 请求到服务器20上，失败两次后，将会停止工作1s，1s之后，再次尝试处理请求，如果失败，再停止1s，如果成功，则正常工作。
    server 192.168.35.20 max_fails=2 fail_timeout=1s;
    server 192.168.35.20;
}
</code></pre>
</li>
<li>keepalive：配置长连接数量，可减少创建连接的消耗，可提高吞吐量<pre><code class="language-bash">upstream tomcats {
        server 192.168.35.20 max_conns=2;
        keepalive 32;
}
server {
        location / {
            proxy_pass http://tomcats;
            # 长连接版本
            proxy_http_version 1.1
            # 清空connection header信息
            proxy_set_header Connection &quot;&quot;;
        }
}
</code></pre>
</li>
</ul>
<h2 id="nginx的缓存">nginx的缓存</h2>
<h3 id="缓存浏览器">缓存浏览器</h3>
<figure data-type="image" tabindex="2"><img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/nginx_browser_cache.png" alt="nginx缓存浏览器" loading="lazy"></figure>
<pre><code class="language-bash">server {
    location / {
        proxy_pass http://tomcats;
        
        # 10s后过期
        expires 10s;
        # 在22.30过期
        expires @22h30m;
        # 缓存在1小时之前失效，不设置cache
        expires -1h;
        # 在很久之前就失效了，不设置cache
        expires epoch;
        # 关闭nginx的缓存
        expires off;
        # 设置最大的缓存时间，会在很长一段时间内都不失效
        expires max;
    }
}
</code></pre>
<h3 id="nginx缓存上游服务器的静态资源">nginx缓存上游服务器的静态资源</h3>
<pre><code class="language-bash">http {
    # proxy_cache_path 设置缓存保存的目录
    # keys_zone 设置共享内存已经占用空间大小
    # max_size 设置缓存大小
    # inactive 超过此时间，缓存自动清理
    # use_temp_path 关闭临时目录
    proxy_cache_path /usr/local/nginx/upstream_cache keys_zone=nginx_cache:5m max_size=1g inactive=30s use_temp_path=off;

    server {
        listen          80;
        server_name     www.daffupman.com;

        # 开启并使用缓存
        proxy_cache nginx_cache;
        # 针对状态码200,304设置缓存过期时间
        proxy_cache_valid 200 304 8h;

        location / {
            proxy_pass http://daff;
            expires 10m;
        }
    }
}
</code></pre>
<h2 id="其他的使用">其他的使用</h2>
<h3 id="配置跨域">配置跨域</h3>
<pre><code class="language-bash">http {
    #允许跨域请求的域，*代表所有
    add_header 'Access-Control-Allow-Origin' *; 
    #允许带上cookie请求 
    add_header 'Access-Control-Allow-Credentials' 'true'; 
    #允许请求的方法，比如 GET/POST/PUT/DELETE 
    add_header 'Access-Control-Allow-Methods' *; 
    #允许请求的header 
    add_header 'Access-Control-Allow-Headers' *;
}
</code></pre>
<h3 id="防盗链">防盗链</h3>
<pre><code class="language-bash">server {
    #对源站点验证 
    valid_referers *.imooc.com; 
    #非法引入会进入下方判断 
    if ($invalid_referer) { 
        return 404; 
}
}
</code></pre>
<h3 id="日志切割">日志切割</h3>
<h4 id="日志切割脚本">日志切割脚本</h4>
<p>在nginx的sbin目录下创建cut_log.sh脚本文件</p>
<pre><code class="language-bash">#!/bin/bash
LOG_PATH=&quot;/var/log/nginx/&quot;
RECORD_TIME=$(date -d &quot;yesterday&quot; +%Y-%m-%d+%H:%M)
PID=/var/run/nginx/nginx.pid
mv ${LOG_PATH}/access.log ${LOG_PATH}/access.${RECORD_TIME}.log
mv ${LOG_PATH}/error.log ${LOG_PATH}/error.${RECORD_TIME}.log
# 向nginxmaster进程发送信号，用户重新打开日志文件
kill -USR1 `cat $PID`
</code></pre>
<h4 id="文件赋权">文件赋权</h4>
<pre><code class="language-shell">chmod +x cut_log.sh
</code></pre>
<h4 id="配置定时任务">配置定时任务</h4>
<pre><code class="language-shell">yum install -y crontabs
#使用 crontab -e 编辑并且添加一行新任务（每分钟执行一次）
*/1 * * * * /usr/local/nginx/sbin/cut_log.sh
# 重启定时任务
service crond restart
# 定时任务常用命令
service crond start //启动服务 
service crond stop //关闭服务 
service crond restart //重启服务 
service crond reload //重新载入配置 
crontab -e // 编辑任务 
crontab -l // 查看任务列表
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL的主从复制]]></title>
        <id>https://daffupman.github.io/YHepijchp/</id>
        <link href="https://daffupman.github.io/YHepijchp/">
        </link>
        <updated>2020-11-23T14:56:10.000Z</updated>
        <content type="html"><![CDATA[<h2 id="主从复制原理">主从复制原理</h2>
<figure data-type="image" tabindex="1"><img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/v2-41b7ab2686ae604c42af50078ed07f6f_1440w.jpg" alt="主从复制原理" loading="lazy"></figure>
<h2 id="搭建一主三从">搭建一主三从</h2>
<h3 id="修改配置文件">修改配置文件</h3>
<pre><code class="language-bash">    vi /etc/my.cnf
</code></pre>
<p>在主节点服务器的配置文件中的mysqld节点下增加配置</p>
<blockquote>
<p>log-bin=mysql-bin #表示启用二进制日志<br>
server-id=43 #表示server编号，编号要唯一</p>
</blockquote>
<p>在每个从节点服务器的配置文件中的mysqld节点下增加配置</p>
<blockquote>
<p>server-id=44</p>
</blockquote>
<h3 id="主节点创建复制日志用户">主节点创建复制日志用户</h3>
<pre><code class="language-bash">    # 创建专用于写binlog的用户
    create user 'writebinlog'@'%' identified by 'writebinlog';
    # 授权
    grant replication slave on *.* to 'writebinlog'@'%';
    # 刷新权限
    flush privileges;
    # 查看主节点状态
    show master status;
    # 如果主节点不是初始状态，需要重置
    reset master
</code></pre>
<h3 id="检查从节点状态">检查从节点状态</h3>
<p>需要确保从节点执行命令 <code>show slave status\G;</code> 时，显示的是：<code>Empty set</code>。如果不是的话，需要执行：</p>
<pre><code class="language-bash">    stop slave; #停止复制，相当于终止从服务器上的IO和SQL线程
    reset slave;
</code></pre>
<h3 id="设置从服务器的master">设置从服务器的master</h3>
<pre><code class="language-bash">    # 在从节点上设置，指定主节点
    change master to master_host='192.168.35.43',master_user='writebinlog',
    master_port=3306,master_password='writebinlog'
    master_log_file='mysql-bin.000001',master_log_pos=156;
    # 开始执行
    start slave;
</code></pre>
<h3 id="检查测试">检查测试</h3>
<p>在从节点上 <code>show slave status</code> 命令观察 <code>Slave_IO_Running</code> 和 <code>Slave_SQL_Running</code> 两个参数是否都是Yes。在主节点上做的操作，都会在从节点上同步的。</p>
<h2 id="双主双从">双主双从</h2>
<h3 id="修改主节点的配置文件">修改主节点的配置文件</h3>
<h4 id="第一台主节点的mysqld下增加以下配置">第一台主节点的mysqld下增加以下配置</h4>
<blockquote>
<p>auto_increment_increment=2  # 步长<br>
auto_increment_offset=1 # 开始的偏移量<br>
log-slave-updates # binlog记录从另一个主节点同步过来的数据<br>
sync_binlog=1 # 几次事务后执行同步</p>
</blockquote>
<h4 id="第一台主节点的mysqld下增加以下配置-2">第一台主节点的mysqld下增加以下配置</h4>
<blockquote>
<p>auto_increment_increment=2  # 步长<br>
auto_increment_offset=1 # 开始的偏移量<br>
log-slave-updates # binlog记录从另一个主节点同步过来的数据<br>
sync_binlog=1 # 几次事务后执行同步</p>
</blockquote>
<h3 id="主节点上复制日志的用户">主节点上复制日志的用户</h3>
<p>在两台主节点上创建writebinlog用户，步骤同上。（需要先停止之前的slave工作）。双主互为主从。</p>
<blockquote>
<p>建议从库开启只读，且连接数据库的账号非root。</p>
</blockquote>
<h3 id="重启">重启</h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redis]]></title>
        <id>https://daffupman.github.io/1d0KrCQDR/</id>
        <link href="https://daffupman.github.io/1d0KrCQDR/">
        </link>
        <updated>2020-11-10T12:27:42.000Z</updated>
        <content type="html"><![CDATA[<h2 id="nosql数据库">NoSQL数据库</h2>
<p>NoSQL,即Not only SQL，不仅仅是SQL，泛指非关系型数据库。NoSQL不依赖业务逻辑方式存储，而以简单的key-value模式存储。因此大大地增加了数据库的扩展能力。NoSQL不遵循SQL标准，不支持ACID，当有着远超SQL的性能。</p>
<p>适用的场景有：对数据高并发的读写、海量数据的读写、对数据高扩展性的以及用不着sql的和用了sql也不行的情况。但不适用于需要事务支持，也不可用于基于sql的存储和查询。</p>
<h2 id="redis">Redis</h2>
<p>Redis是一个开源的key-value存储系统。Redis支持各种数据结构，包括string、list、set、zset和hash。这些数据类型都支持push/pop、add/remove及取交集并集和差集等丰富的操作，且这些操作都是原子性的。在此基础上，Redis支持各种不同方式的排序。为保证效率，Redis的数据都缓存在内存中，同时也会周期性地把数据写入磁盘或把修改操作写入记录文件，在此基础上实现master-slave同步。</p>
<p>Redis是单线程+IO多路复用技术。多路复用是指使用一个线程来检查多个文件描述符（Socket）的就绪状态，比如调用select和poll函数，传入多个文件描述符，如果有一个文件描述符就绪，则返回，否则阻塞直到超时。得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动线程执行。</p>
<h3 id="安装">安装</h3>
<ol>
<li>下载地址：https://download.redis.io/releases/redis-5.0.5.tar.gz</li>
<li>在/opt目录下解压缩<pre><code class="language-bash">tar -zxvf redis-5.0.5.tar.gz
</code></pre>
</li>
<li>安装gcc环境<pre><code class="language-bash">yum -y install gcc-c++
</code></pre>
</li>
<li>在redis目录下编译运行<pre><code class="language-bash">make &amp;&amp; make install
</code></pre>
</li>
<li>配置文件到/etc/redis/下<pre><code class="language-bash">cp /opt/redis-5.0.5/redis.conf /etc/redis/
</code></pre>
</li>
<li>修改配置文件<pre><code class="language-bash">vim /etc/redis/redis.conf
</code></pre>
<blockquote>
<p>#修改daemonize为yes，后台运行<br>
daemonize yes<br>
#所有机器可连接<br>
bind 0.0.0.0<br>
#注释掉保护模式<br>
#protected-mode yes<br>
#设置登录密码<br>
requirepass root<br>
#设置工作目录<br>
dir /usr/local/redis</p>
</blockquote>
</li>
<li>开机自启
<ol>
<li>拷贝启动脚本到 <code>/etc/init.d</code> 下<pre><code class="language-bash">cp /opt/redis-5.0.5/utils/redis_init_script /etc/init.d
</code></pre>
</li>
<li>编辑脚本文件<pre><code class="language-bash">vim /etc/init.d/redis_init_script
</code></pre>
<blockquote>
<p>########### 第二行添加以下文字 ############<br>
#chkconfig: 22345 10 90<br>
#description: start redis at boot time<br>
#修改CONF属性值为redis自定义的配置<br>
CONF=&quot;/etc/redis/redis.conf&quot;</p>
</blockquote>
</li>
<li>加入系统自启<pre><code class="language-bash">chkconfig --add redis_init_script
</code></pre>
</li>
<li>开启脚本<pre><code class="language-bash">chkconfig redis_init_script on
</code></pre>
</li>
</ol>
</li>
<li>重启redis<pre><code class="language-bash">./etc/init.d/redis_init_script start
</code></pre>
</li>
</ol>
<h3 id="redis五大数据类型">Redis五大数据类型</h3>
<h4 id="string类型">String类型</h4>
<p>String是最基本的类型，采用key-value形式。String类型使二进制安全的，redis的String可以包含任何数据，如图片或序列化对象。一个Redis中字符串value最多可以是512M。使用示例：</p>
<blockquote>
<p>get <key>				# 查询对应的键值<br>
set <key> <value>		# 添加键值对<br>
append <key> <value>	# 将给定的value追加到原值的末尾<br>
strlen <key>			# 获得值得长度<br>
setnx <key> <value>		# key不存在时设置key<br>
incr <key>				# key值加一，如果key为null，key则为1<br>
decr <key>				# key值建议，如果key为null，key则为-1<br>
incrby/decrby <key> &lt;步长&gt;	#key值按指定步长变化<br>
mset <key1> <val1> <key2> <val2> ...# 同时设置一个或多个key-val对<br>
mget <key1> <val1> <key2> <val2> ...# 同时获取一个或多个value<br>
msetnx <key1> <val1> <key2> <val2>	# 同时设置一个或多个key-val对，当且仅当key都不存在<br>
getrange <key> <start> <end>	# 获得值得范围<br>
setrange <keu> <start> <value>	# 从start位置开始，用val覆写key所存储的值<br>
setex <key> <time> <value>		# 设置key-val，同时加上过期时间<br>
getset <key> <val>				# 设置新值，返回旧值</p>
</blockquote>
<h4 id="list类型">List类型</h4>
<p>单值多键。Redis列表是简单的字符串列表，按照插入顺序排序。key从列表的头部（左边）或尾部（右边）插入。List底层是双向链表，两端操作性能很高，中间的节点相对较差。</p>
<blockquote>
<p>lpush/rpush <key> <val1> <val2>...   # 从左边/右边插入一个或多个值<br>
lpop/rpop <key>  # 从左边/右边吐出一个值，值在键在，值光键亡<br>
rpoplpush <key1> <key2>  # 从key1列表右边吐出一个值，从左边插入&gt; key2<br>
lrange <key> <start> <stop>  # 按照索引下标获得元素（从左到右）<br>
lindex <key> <index>  # 按照索引下标获得元素（从左到右）<br>
llen <key>  # 获得列表长度<br>
linsert <key> before <val> <newVal>	 # 在val后面插入newValue<br>
lrem <key> <n> <val>	 # 从左边删除n个val</p>
</blockquote>
<h4 id="set类型">Set类型</h4>
<p>set可以自动去重，并判断某个成员是否在一个set集合类。set是String类型的无序集合，底层为一个value为null的hash表，所以删除、查找的复杂度都是O(1)。</p>
<blockquote>
<p>sadd <key> <val1> <val2>...			# 将多个元素放入到key集合中，相同的则忽略<br>
smembers <key>						# 取出该集合的所有值<br>
sismember <key> <val>				# 判断key集合是否有val值，有返1，无返0<br>
scard <key>							# 返回该集合的元素个数<br>
srem <key> <val1> <val2>			# 删除集合种的元素<br>
spop <key>							# 随机吐出一个值<br>
srandmember <key> <n>				# 随机从key集合中取出n个值，不会删除<br>
sint <key1> <key2>					# 返回两个集合的交集元素<br>
sunion <key1> <key2>				# 返回两个集合的并集元素<br>
sdiff <key1> <key2>					# 返回两个集合的差集元素</p>
</blockquote>
<h4 id="hash类型">Hash类型</h4>
<p>Hash是一个string类型的field和value的映射表，HashMap适合用于存储对象。使用示例：</p>
<blockquote>
<p>hset <key> <field> <value>		# 给key集合中的field键赋值value<br>
hget <key1> <field>				# 从key1集合field取出value<br>
hmset <key1> <field1> <val1> <field2> <val2>	# 批量设置hash的值<br>
hexists key <field>				# 查看hash表key中，给定field是否存在<br>
hkeys <key>						# 列出该hash集合的所有field<br>
hvals <key>						# 列出该hash集合的所有value<br>
hincrby <key> <field> <increment>	# 为hash表key中的域field的值加上increment<br>
hsetnx <key> <field> <value>	# 当field不存在时，将key中的field值设置为value</p>
</blockquote>
<h4 id="zset类型">zset类型</h4>
<p>zset是一个没有重复元素的字符串集合。与set集合不同的是，有序集合的每个成员都关联了一个评分，这个评分score被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复的。</p>
<p>因为元素是有序的，可以很快的根据评分或次序来获取一个范围的元素。访问有序集合的中间元素也很快。</p>
<p>使用示例：</p>
<blockquote>
<p>zadd <key> <score1> <val1> <score2> <val2>...	# 将一个或多个元素及其score值加入到有序集key中<br>
zrang <key> <start> <stop> [withscores]			# 返回有序集key中，下标在<start>和<stop>之间的元素（含scores）<br>
zrangebyscore key min max [withscores] [limit offset count]	# 返回有序集key中，所有score值介于min和max之间（包含）的成员，有序集成员按score值递增排列<br>
zrevrangebyscore key max min [withscores] [limit offset count]	# 同上，逆序<br>
zincrby <key> <incrment> <val>	# 为元素的score加上增量<br>
zrem <key> <val>	# 删除该集合下指定值得元素<br>
zcount <key> <min> <max>	#统计该集合，分数区间内的元素个数<br>
zrank <key> <val>	# 返回该值在集合中的排名（基于0）</p>
</blockquote>
<h2 id="redis事务">Redis事务</h2>
<p>Redis事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。</p>
<h3 id="特性">特性</h3>
<ol>
<li>单独的隔离操作</li>
<li>没有隔离级别概念</li>
<li>不保证原子性</li>
</ol>
<h3 id="指令">指令</h3>
<ul>
<li>multi：开启multi后，之后的命令入队，但不会执行；</li>
<li>exec：执行入队的命令；</li>
<li>discard：在入队的过程中可以使用discard放弃组队正常的事务执行流程。<br>
当组队阶段中某个命令出现了错误，在exec时整个队列都会被取消。如果执行阶段某个命令报错，只有报错的命令不会被执行，而其他的命令都会执行，不会回滚。</li>
</ul>
<h3 id="事务的冲突和解决方式">事务的冲突和解决方式</h3>
<p>事务中可能存在的多个指令同时修改某个共享资源，但实际上有限的共享资源是不足以满足那些所有的指令的。这时候就是一个事务冲突的现象。一般可以借助悲观锁或乐观锁的思想解决。</p>
<h4 id="悲观锁pessimistic-lock">悲观锁（Pessimistic Lock）</h4>
<p>悲观锁认为每次对数据的操作都是会修改的，所有每次都会加上锁。在传统的关系型数据库中用的很多，比如：行锁、表锁、读锁、写锁等。</p>
<h4 id="乐观锁optimistic-lock">乐观锁（Optimistic Lock）</h4>
<p>与悲观锁相反，乐观锁认为每次对数据的操作都不会修改，所以并不会上锁，但会在数据更新的时候同时判断此期间原来的数据有没有被修改，一般使用版本号机制（check-and-set）。</p>
<p>在redis也有应用乐观锁的相应的指令：watch。在执行multi指令之前，先使用watch命令监视一个或多个key。</p>
<pre><code class="language-bash">watch k1 [k2, ...]
</code></pre>
<p>在事务执行（exec）之前，只要这些被监视的key有所改动，那么事务将被打断。也可使用unwatch取消对key的监视。</p>
<h2 id="redis的持久化">Redis的持久化</h2>
<p>redis是一种内存数据库，但也提供了持久化的技术。主要有RDB和AOF。</p>
<h3 id="rdbredis-database">RDB（Redis Database）</h3>
<p>在指定的时间内将内存中的数据集快照（snapshot）写入磁盘，恢复时将快照文件读入内存。</p>
<p>RDB的过程大致是这样的：Redis会单独创建（fork）一个子进程做持久化。子进程先将数据写入到一个临时文件中，待持久化结束后，再用这个临时文件替换上一次持久化好的文件。整个过程中，主进程不进行任何IO操作，确保了极高的性能。如果需要大规模数据的恢复，且对于数据的恢复的完整性不高，那RDB会比AOF更高效，但缺点就是丢失最后一次持久化的数据。</p>
<blockquote>
<p>关于fork：<br>
在linux系统中，调用fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调&gt; 用。出于效率考虑，Linux中引入“写时复制”技术，一般情况父进程和子进程会共用一段物理内存，只有&gt; 进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。</p>
</blockquote>
<h4 id="配置文件相关的配置项">配置文件相关的配置项</h4>
<blockquote>
<p>#db保存的文件名称<br>
dbfilename dump.rdb<br>
#rdb文件的保存的路径<br>
dir ./<br>
#rdb的保存策略<br>
save 900 1<br>
save 300 10<br>
save 60 10000<br>
#当redis无法写入磁盘的话，直接关闭redis的写操作。可能造成数据不一致<br>
stop-writes-on-bgsave-error yes<br>
#进行RDB保存时将文件压缩，会消耗一些cpu的资源<br>
rdbcompression yes<br>
#关闭rdb存储数据时的数据校验，可提升性能，开启的话会有10%的性能损耗<br>
rdbchecksum no</p>
</blockquote>
<h4 id="备份与恢复">备份与恢复</h4>
<h5 id="备份">备份</h5>
<pre><code class="language-bash">config get dir	# 查询RDB文件的路径
cp *.rdb /target	# 复制备份文件到其他目录
</code></pre>
<h5 id="恢复">恢复</h5>
<pre><code class="language-bash">cp /target/* /redis	# 将备份的文件复制到dir下，redis启动时会自动加载
</code></pre>
<h4 id="rdb的优点和缺点">RDB的优点和缺点</h4>
<ul>
<li>优点
<ul>
<li>每隔一段时间全量备份数据</li>
<li>灾备简单，可远程传输</li>
<li>子进程备份的时候，主进程不会有任何io，保证数据的完整性</li>
<li>相对于aof，可以快速重启恢复大文件</li>
</ul>
</li>
<li>缺点
<ul>
<li>相对于aof，可以快速重启恢复大文件</li>
<li>子进程会和父进程占用的大小一样的内存，cpu压力大</li>
<li>不能实时备份</li>
</ul>
</li>
</ul>
<h3 id="aofappend-of-file">AOF（Append Of File）</h3>
<p>以日志的形式来记录每个写操作。将redis执行的每一个写指令（读指令不管）记录在文件里，该文件只能追加，不能更改。redis在启动之初，会读取日志文件，重新执行所有的命令。AOF默认不开启，需要修改配置文件。</p>
<blockquote>
<p>#开启aof<br>
appendonly yes<br>
#指定aof文件，路径与rdb一致<br>
appendfilename &quot;appendfileonly.aof&quot;</p>
</blockquote>
<p>apf的备份与恢复与rdb一样。当同时开启aof和rdb，系统默认读取aof的数据。如果aof文件损害，使用以下命令恢复：<code>redis-check-aof --fix appendonly.aof</code>。</p>
<h4 id="aof的配置">aof的配置</h4>
<blockquote>
<p>#appendfsync always<br>
appendfsync everysec	# 不确定则使用这个，每秒进行一次备份<br>
#appendfsync no</p>
<p>#重写的时候是否要同步，no可以保证数据安全<br>
no-appendfsync-on-rewrite no</p>
<p>#重写机制：避免文件越来越大，自动优化压缩指令，会fork一个新的进程去完成重写动作，新进程里的内存数据会被重写，此时旧的aof文件不会被读取使用，类似rdb<br>
#当前AOF文件的大小是上次AOF大小的100% 并且文件体积达到64m，满足两者则触发重写<br>
auto-aof-rewrite-percentage 100<br>
auto-aof-rewrite-min-size 64mb</p>
</blockquote>
<h4 id="aof的优点和缺点">aof的优点和缺点</h4>
<ul>
<li>优点：
<ul>
<li>aof操作可以增加数据的可靠性和完整性，还可以异步操作；</li>
<li>以日志的形式追加，在磁盘满的时候，会执行redis-check-aof；</li>
<li>数据大的时候，redis会自动重写aof，重写是安全的，不会影响到客户端的读写；</li>
</ul>
</li>
<li>缺点：
<ul>
<li>相同的数据，aof会比rdb占用的内存大；</li>
<li>aof的同步机制会不如rdb，aof每秒都做备份写数据，相对于rdb来说速度较慢。以fsync备份的话，可以提升速度，但也会影响cpu性能。</li>
</ul>
</li>
</ul>
<h3 id="rdb和aof选择问题">RDB和AOF选择问题</h3>
<p>RDB适合大量数据的恢复，但数据的完整性和一致性可能会不足。RDB可能会丢失最后一次的备份，作为缓存来说，是可以忍受的。但是缓存的数据是要保证数据的完整性，需要使用AOF。</p>
<ul>
<li>如果可以接受一段时间的缓存丢失，可以使用RDB</li>
<li>如果需要保证实时性的数据，使用AOF</li>
<li>RDB和AOF可以一起使用，RDB做冷备，可在不同时期对不同版本做备份，AOF做热备，数据可以只有1秒的丢失。当AOF不可用的使用，可以再用RDB恢复。也就是Redis的恢复会先加载AOF，如果AOF有问题，会加载RDB。</li>
<li>如果只做缓存，都不用</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MySQL的索引及优化]]></title>
        <id>https://daffupman.github.io/WZxqM5YDi/</id>
        <link href="https://daffupman.github.io/WZxqM5YDi/">
        </link>
        <updated>2020-11-07T13:21:55.000Z</updated>
        <content type="html"><![CDATA[<p>对于MySQL来说，可以使用sql语句查询出我们需要的数据，这是最基本的功能。在表中数据量比较小的情况下，SQL的性能差异并不明显，但是在表数据量比较大的时候，SQL执行的效率就有明显的时间差异。这里会记录我们如何发现SQL的性能问题，如何优化SQL以及优化的底层原理。</p>
<h2 id="sql优化">SQL优化</h2>
<h3 id="索引">索引</h3>
<h4 id="主键索引和非主键索引">主键索引和非主键索引</h4>
<p>非主键索引的叶子节点存放的是主键的值，而主键索引的叶子节点存放的是整行数据。非主键索引也被称为二级索引，而主键索引也被称为聚簇索引。</p>
<p>如果条件查询使用的是主键索引，那么直接冲索引树上获取整行数据，而条件查询的非主键索引，那么需要从节点上获取主键值，然后再去主键索引树上获取完整的数据。即会发生回表操作。</p>
<p>聚集索引表示表中存储的数据按照索引的顺序存储，检索效率比非聚集索引高，但对数据更新影响较大。非聚集索引表示数据存储在一个地方，索引存储在另一个地方，索引带有指针指向数据的存储位置。非聚集索引检索效率比聚集索引低，但对数据更新影响较小。</p>
<p>使用主键自增的索引，可以避免B+树重新排序和页分裂操作。</p>
<h3 id="测试数据准备">测试数据准备</h3>
<ul>
<li>测试数据文件：<code>https://github.com/datacharmer/test_db</code></li>
<li>导入数据<pre><code class="language-bash">mysql -uroot -proot &lt; employees.sql
</code></pre>
</li>
<li>测试数据导入是否成功<pre><code class="language-bash">mysql -uroot -proot -t &lt; test_employees_md5.sql
</code></pre>
</li>
</ul>
<h3 id="如何找出效率低的sql">如何找出效率低的SQL</h3>
<p>要解决SQL效率低的问题就是如何发现哪些SQL导致了执行效率。常用的方法就是 <code>打开慢查询日志</code> ，命令行方式如下：</p>
<pre><code class="language-bash"># 开启/关闭慢查询日志，默认off
set global slow_query_log = ON/OFF
# 设置慢查询日志的输出位置
set global slow_query_log_file = log/slow_query.log
# 设置慢查询的时间阀值，默认10秒，需要重新打开会话可见。测试的时候一般设置成1秒
set global long_query_time = 1
# 开启/关闭 记录没有使用索引的sql
set global log_queries_not_using_indexes = ON/OFF
</code></pre>
<p>配置文件方式如下（需要重启）：</p>
<pre><code class="language-bash">[mysqld]
log_output='FILE,TABLE'
slow_query_log=ON
long_query_time=0.001
slow_query_log_file=log/slow_query.log
</code></pre>
<blockquote>
<p>其他相关参数与默认值<br>
log_output：日志输出的地方。默认FILE，即文件。可以设成TABLE，则日志记录到mysql.slow_log中。也可以设置多种格式，如：FILE，TABLE。<br>
long_query_time：执行时间超过多久记录到慢查询日志，单位秒，默认10。<br>
log_queries_not_using_indexes：是否要将未使用索引的SQL记录到慢查询日志中，默认OFF。建议在开发环境开启，生产环境打开。<br>
log_throttle_queries_not_using_indexes：log_queries_not_using_indexes配置项开启生效，用于限制每分钟写入没有使用慢sql的数量，默认0。<br>
min_examined_row_limit：扫描行数至少达到多少行会记录到慢查询日志，默认0。<br>
log_slow_amdin_statements：是否要记录管理语句，默认关闭。管理语句包括：alter table，analyze table，check table，create index，drop index，optimize table，repair table。默认off。<br>
slow_query_log_file：指定慢查询日志文件路径，默认/var。<br>
log_slow_slave_statements：该参数在从库上设置，决定是否记录在复制过程中超过long_query_time的SQL，如果binlog格式是row，则改参数无效。<br>
log_slow_extra：当log_output=FILE，是否要记录额外信息（MySQL8.0.14），对log_output=TABLE无影响。</p>
</blockquote>
<p>这样所有的慢查询sql都会记录到 <code>log/slow_query.log</code> 文件里了。我们可以直接阅读文件里的内容，来分析sql的执行问题。但是一般都是借助工具来分析，主要有两种工具：</p>
<ul>
<li>mysql的官方工具 <code>mysqldumpslow</code>，使用命令：<pre><code class="language-bash">mysqldumpslow [OPTS...][LOGS...]
</code></pre>
示例：
<blockquote>
<p>mysqldumpslow -s t -t 10 -a db-slow.log</p>
</blockquote>
</li>
<li>percona工具，这是第三方的，需要先下载(https://www.percona.com/downloads/percona-toolkit/3.2.1/binary/redhat/7/x86_64/percona-toolkit-3.2.1-1.el7.x86_64.rpm)：<pre><code class="language-bash">yum install -y perl-DBD-MySQL.x86_64 perl-DBI.x86 perl-Time-HiRes.x86_64 perl-IO-Socket-SSL.noarch perl-TermReadKey.x86_64 perl-Digest-MD5
rpm -ivh percona-toolkit-3.2.1-1.el7.x86_64.rpm
</code></pre>
</li>
</ul>
<blockquote>
<p>数据库实时监控sql语句：</p>
<pre><code class="language-mysql">  SELECT id, `user`, `host`, DB, command, `time`, state, info 
  FROM information_schema.PROCESSLIST
  WHERE TIME &gt;= 60;
</code></pre>
</blockquote>
<h3 id="分析执行计划">分析执行计划</h3>
<h4 id="explain">EXPLAIN</h4>
<p>在获取到耗时较长的sql或我们刚写sql性能如何的时候，我们可以使用 <code>explain + SQL</code> 来输出sql的执行计划。<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/20201022224740.png" alt="explain语法" loading="lazy"><br>
执行某条sql输出的执行计划示例：<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/20201025213701.png" alt="示例执行计划" loading="lazy"><br>
可以看出执行计划有很多列，这里给出各个列的大致解释：</p>
<ul>
<li><code>id</code>：sql执行的顺序。id的值为一个数字或null。当id为一个数字的时候，表示查询执行的顺序；当id为null时，表示此行数据是由其他查询通过union出来的结果集。id值相同时，由上到下执行，id值不同，由大到小执行。</li>
<li><code>select_type</code>：查询类型，可选的值有：
<ul>
<li>SIMPLE：不包含子查询或是UNION操作的查询，多表JOIN也是SIMPLE；</li>
<li>PRIMARY：查询中如果包含任何子查询，那么最外层的查询则被标记为PRIMARY；</li>
<li>SUBQUERY：SELECT列表中的子查询；</li>
<li>DEPENDENT SUBQUERY：依赖外部结果的子查询；</li>
<li>UNION：union操作的第二个或是之后的查询的值为union；</li>
<li>DEPENDENT UNION：当union做为子查询时，第二个或第二个后的查询的select_type值；</li>
<li>UNION RESULT：UNION产生的结果集；</li>
<li>DERIVED：出现在FROM子句中的子查询。</li>
</ul>
</li>
<li><code>table</code>：指明从哪个表中获取数据。&lt;unionM,N&gt;由id为M和N查询union产生的结果集。&lt;derived N&gt;/&lt;subquery N&gt; 由id为n的查询产生的结果。</li>
<li><code>partitions</code>：对于分区表，显示查询的分区id；对于非分区表，显示NULL；</li>
<li><code>type</code>：索引使用的类型。<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/20201025223733.png" alt="type列的值" loading="lazy"></li>
<li><code>possible_keys</code>：查询中可能会用到的索引；</li>
<li><code>key</code>：查询是实际用到的索引；</li>
<li><code>key_len</code>：实际使用索引的最大长度，由字段长度决定的；</li>
<li><code>ref</code>：表示哪些列或常量被用于索引查找；</li>
<li><code>rows</code>：根据统计信息预估的扫描的行数；</li>
<li><code>filtered</code>：表示返回结果的行数占需读取行数的百分比；</li>
<li><code>extra</code>：sql查询的一些额外的信息，可能显示的如下
<ul>
<li>distinct：优化distinct操作，在找到第一匹配的元组后即停止；</li>
<li>not exists：使用not exists来优化查询；</li>
<li>using filesort：使用文件来进行排序，通常会出现在order by或group by查询中；</li>
<li>using index：使用了覆盖索引进行查询；</li>
<li>using temporary：MySQL需要使用临时表来处理查询，常见于排序，子查询和分组查询；</li>
<li>using where：需要在MySQL服务器层使用where条件来过滤数据；</li>
<li>select tables optimized away：直接通过索引来获得数据，不用访问表。</li>
</ul>
</li>
</ul>
<h4 id="show-profiles">SHOW PROFILES</h4>
<p>进行更细致的SQL性能分析。</p>
<pre><code class="language-bash">select @@have_profiling;    # 查看是否支持profile功能
select @@profiling; # 查看是否开启profile功能
set @@prfiling=1;   # 开启profile功能
set profiling_history_size=100; # 设置profiling处理的sql数量，默认15条
show profiles;
show profile for [all] query &lt;id&gt;;    # 查询指定id的sql执行详情
</code></pre>
<h4 id="optimizer-trace">optimizer trace</h4>
<h3 id="如何优化sql">如何优化sql</h3>
<p>可以从优化sql查询所涉及到的表中的索引或改写sql以达到更好的利用索引的目的。索引可以告诉存储引擎如何快速的查询到所需要的数据。</p>
<p>索引的数据结构：</p>
<ul>
<li>B树<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/20201114201514.png" alt="B树" loading="lazy">
<ul>
<li>根节点的子节点个数：2&lt;=x&lt;=m，m是树的阶；</li>
<li>中间节点的子节点个数：m/2 &lt;= y&lt;= m；</li>
<li>每个中间节点包含n个关键字，n=子节点个数-1且按升序排序</li>
<li>在关键字节点的左子树都是小于关键字的，右子树都是大于关键字的。</li>
</ul>
</li>
<li>B+树<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/20201114203716.png" alt="B+树" loading="lazy">
<ul>
<li>有n个子节点的节点中含有n个关键字；</li>
<li>叶子节点包含了所有关键字的信息，且关键字自小到大排序，形成一个有序链表；</li>
<li>非叶子节点只保存索引，叶子节点保存索引和数据。</li>
</ul>
</li>
<li>B树与B+树
<ul>
<li>B+树相对矮胖；</li>
<li>B树可以直接在非叶子节点上获取数据，B+树需要到叶子节点上才能获取数据；</li>
<li>B+树可以在范围查询的时候，在查询到某叶子节点后按链表向后遍历，效率会更高。</li>
</ul>
</li>
</ul>
<p>InnoDB的存储方式：</p>
<ul>
<li>B+树</li>
<li>主键索引：叶子节点存储主键和数据</li>
<li>非主键索引（二级索引、辅助索引）：叶子节点存储索引以及主键</li>
</ul>
<p>MyISAM存储方式：</p>
<ul>
<li>B+树</li>
<li>主键/非主键索引的叶子都是存储指向数据块的指针</li>
</ul>
<p>InnoDB支持的索引类型有：</p>
<ul>
<li>Btree索引：平常使用中，如果没有说明索引的类型，一般都是Btree索引；
<ul>
<li>使用B+树的结构存储索引数据；</li>
<li>适用于前缀查询</li>
<li>适用于全值匹配的查询
<blockquote>
<p>class_name = 'mysql' 或 class_name in ('mysql', 'postgreSQL')</p>
</blockquote>
</li>
<li>适合处理范围查找
<blockquote>
<p>study_cnt between 1000 and 3000 或 study_cnt &gt; 3000</p>
</blockquote>
</li>
<li>Btree索引的从索引的最左侧开始匹配查找列</li>
</ul>
</li>
<li>自适应Hash索引：为优化查询性能自动建立的，不需要开发人员或DBA手动管理；Hash索引的性能会比B树好些，但是Hash索引不是按照索引值排序，所以不可使用排序。也不支持部分索引列匹配查询，只支持等值查询，不支持范围查询，模糊查询。Hash冲突越严重，性能下降越厉害。
<blockquote>
<p>innodb_adaptive_hash_index<br>
为什么用 B+ 树做索引而不用哈希表做索引?</p>
<ol>
<li>哈希表是把索引字段映射成对应的哈希码然后再存放在对应的位置，这样的话，如果我们要进行模糊查找的话，显然哈希表这种结构是不支持的，只能遍历这个表。而B+树则可以通过最左前缀原则快速找到对应的数据。</li>
<li>如果我们要进行范围查找，例如查找ID为100 ~ 400的人，哈希表同样不支持，只能遍历全表</li>
<li>索引字段通过哈希映射成哈希码，如果很多字段都刚好映射到相同值的哈希码的话，那么形成的索引结构将会是一条很长的链表，这样的话，查找的时间就会大大增加。</li>
</ol>
</blockquote>
</li>
<li>全文索引：使用elasticsearch或solr</li>
<li>空间索引：存储GIS数据，地理位置的索引（不推荐）</li>
</ul>
<p>应该在什么列上建立索引？</p>
<ol>
<li>在where子句中的列，这些列是否有比较好的筛选性（列值的重复越小的，筛选性越好）</li>
<li>包含在ORDER BY、GROUP BY、DISTINCT中的字段</li>
<li>多表JOIN的关联列
<blockquote>
<p>创建索引：create index idx_xx ON table_name(column_name)<br>
查看索引：show index from table_name<br>
删除索引：alter table table_name drop index idx_xx</p>
</blockquote>
</li>
</ol>
<p>如何选择符合索引键的顺序？</p>
<ol>
<li>区分度最高的列放在联合索引的最左侧</li>
<li>使用最频繁的列放到联合索引的最左侧</li>
<li>尽量把字段长度小的列放在联合索引列的最左侧</li>
</ol>
<p>Btree索引的限制：</p>
<ul>
<li>只能从最左侧开始按索引键的顺序使用索引，不能跳过索引键</li>
<li>NOT IN和&lt;&gt;操作无法使用索引</li>
<li>索引列上不能使用表达式或者函数</li>
</ul>
<p>索引并非越多越好，索引可以提高查询效率，也会降低插入和更新的效率。使用IN查询是可以用到索引，但是IN查询列表很大，mysql引擎会认为使用全表扫描的效率更高而放弃使用索引。查询过滤的顺序也不必与索引键顺序相同才可以使用到索引，mysql引擎会自动调整顺序以适应索引顺序从而使用到索引。</p>
<h3 id="改写sql">改写SQL</h3>
<p>改写SQL的原则：</p>
<ul>
<li>使用outer join代替not in（MySQL8.0会自动转换）</li>
<li>使用CTE代替子查询</li>
<li>拆分复杂的大SQL为多个简单的小SQL</li>
<li>使用计算列优化查询<pre><code class="language-bash"># 可以生成一个total_score计算列。为计算列添加索引，可优化查询速度。
  alter table imc_classvalue add column total_score DECIMAL(3,1) as (content_score + level_score + logic_score) 
</code></pre>
</li>
</ul>
<h4 id="join语句的优化">JOIN语句的优化</h4>
<p>JOIN的类型有：<br>
<img src="https://for-markdown.oss-cn-shanghai.aliyuncs.com/sql-join.png" alt="JOIN的类型" loading="lazy"></p>
<p>JOIN的算法：</p>
<ul>
<li>Nested Loop Join（NLJ）</li>
<li>Block Nested Loop Join（BNLJ）：使用join buffer降低对一张表的扫描次数，可以使用 <code>join_buffer_size</code>调整join_buffer大小；每个能被缓存的join都会分配一个join buffer，一个查询可能拥有多个join buffer。join buffer在执行连接之前会分配，在查询完成之后释放。但需要满足以下条件：
<ul>
<li>连接类型是ALL，index或是range；</li>
<li>第一个nonconst table不会分配join buffe，即使类型是all或index；</li>
<li>join buffer只会缓存需要的字段，非整行数据；</li>
</ul>
</li>
<li>Batched Key Access Join（BKA）：mysql5.6，基于Multi Range Read（MRR）。
<blockquote>
<p>MRR核心：将随机IO转换成顺序IO，从而提升性能。<br>
MRR参数：<br>
- optimizer_switch的子参数<br>
- mrr：是否开启mrr，on开启，off关闭<br>
- mrr_cost_based：表示是否要开启基于成本计算的MRR<br>
- batched_key_access：<br>
- read_rnd_buffer_size：指定mrr缓存大小</p>
</blockquote>
</li>
<li>Hash Join：mysql8.0.18，当限制会比较多，在mysql8.0.20限制会比较少，建议在8.0.20上使用。用来代替BNLJ。为较小的表在内存中构建一个hash表，然后去扫描大表，扫描的过程中查询hash表是否满足条件。</li>
</ul>
<p>JOIN调优原则：</p>
<ul>
<li>小表驱动大表（优化器会自动执行，也可以使用 straight_join 指定驱动表）；</li>
<li>如果where条件，应当要能够使用索引，并尽可能地减少外层循环的数据量；</li>
<li>join的字段尽量创建索引；</li>
<li>尽量减少扫描的行数（限制在百万内）；</li>
<li>参与join的表不要太多（不超过3张）；</li>
<li>如果被驱动表的join字段用不了索引，且内存较为充足，可以考虑join buffer设置的大一些。</li>
</ul>
<p>JOIN调优原则：</p>
<ul>
<li>小表驱动大表（优化器会自动执行，也可以使用 straight_join 指定驱动表）；</li>
<li>如果where条件，应当要能够使用索引，并尽可能地减少外层循环的数据量；</li>
<li>join的字段尽量创建索引；</li>
<li>尽量减少扫描的行数（限制在百万内）；</li>
<li>参与join的表不要太多（不超过3张）；</li>
<li>如果被驱动表的join字段用不了索引，且内存较为充足，可以考虑join buffer设置的大一些。</li>
</ul>
<h4 id="分页查询优化">分页查询优化</h4>
<ol>
<li>使用覆盖索引</li>
<li>使用覆盖索引 &amp; join</li>
<li>使用覆盖索引 &amp; 子查询</li>
<li>范围查询 &amp; limit语句</li>
<li>如果可以获取起始主键值和结束主键值，可以改成between...and</li>
<li>禁止过大页码</li>
</ol>
<h4 id="count优化">COUNT优化</h4>
<p>一些关于count的结论：</p>
<ol>
<li>count(*)和count(1)是一样的；</li>
<li>count(*)会选择最小的非主键索引，如果不存在任何非主键索引，则会使用主键；</li>
<li>count(*)不会排除null行，而count(字段)会排除；</li>
<li>如果没有特殊需求，尽量count(*)。</li>
</ol>
<p>优化建议：</p>
<ul>
<li>mysql8.0.13会针对无条件的count语句去优化</li>
<li>使用更小的非主键索引</li>
<li>把数据库引擎切换成MyISAM</li>
<li>使用并维护一张汇总表，</li>
<li>使用sql_calc_found_rows，未来会被废弃</li>
<li>使用缓存</li>
<li>查询information_schema.tables，估算值</li>
<li>show table status where name = 'user'，估算值</li>
<li>explain select * from salaries，估算值</li>
</ul>
<h4 id="order-by优化">ORDER BY优化</h4>
<p>MySQL的排序模式</p>
<ol>
<li>
<p>rowid排序（常规排序）</p>
<ol>
<li>过程
<ol>
<li>从表中获取满足where条件的记录；</li>
<li>对于每条记录，将记录的主键即排序键（id、order_column）取出放入sort buffer（<code>sort_buffer_size</code>）；</li>
<li>如果sort buffer能存放所有满足条件的id、order_column，则进行排序；否则sort buffer满后，排序并写到临时文件。快排算法；</li>
<li>若排序中产生了临时文件，需要利用归并排序算法，从而保证记录有序；</li>
<li>循环执行上述过程，知道所有满足条件的记录全部参与排序；</li>
<li>扫描排好序的id、order_colum对，并利用id去取select需要返回的其他字段；</li>
<li>返回结果集</li>
</ol>
</li>
<li>特点
<ol>
<li>如果sort buffer不能存放结果集里面的所有id、order_column，会产生临时文件</li>
<li>一次排序需要两次IO。
<blockquote>
<p>第二步：把id、order_column放入sort_buffer；第六步：通过id去获取需要返回的其他字段。由于返回结果是按照order_column排序的，所以id是乱序的，会存在随机io的问题。MySQL内部针对这种情况做了优化。在用id取数据之前，会按照id排序并放到一个缓存里面，这个缓存大小由<code>read_rnd_buffer_size</code> 控制，接着再去记录，从而把随机io转换成顺序io。</p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
<li>
<p>全字段排序（优化排序）<br>
直接取出SQL中需要的所有的字段，放到sort buffer中。由于sort buffer已经包含了查询需要的所有字段，因此在sort buffer中排序完成后可直接返回。<br>
相对于rowid排序，全字段排序的优缺点有：</p>
<ul>
<li>性能提升，无需两次io；</li>
<li>一行数据占用的空间回比rowid排序的要多，更容易导致临时文件。</li>
</ul>
</li>
<li>
<p>rowid和全字段是如何选择的？<br>
当order by SQL中出现字段的总长度小于配置项 <code>max_length_for_sort_data</code> ，会使用全字段排序，否则使用rowid排序。</p>
</li>
<li>
<p>打包字段排序<br>
MySQL5.7引入，全字段模式的优化，工作原理一样，但是将字段紧密地排列在一起，而不是使用固定长度空间。</p>
</li>
</ol>
<p>如何调优order by：</p>
<ul>
<li>利用索引，防止filesort发生</li>
<li>如果发生filesort，并且没法避免，需要优化filesort
<ul>
<li>调大sort_buffer_size，减少/避免临时文件和归并操作
<ul>
<li>optimizer trace中 <code>num_initial_chunks_spilled_to_disk</code>的值</li>
<li>sort_merge_passess变量的值</li>
</ul>
</li>
<li>调大 <code>read_rnd_buffer_size</code>，让一次顺序io返回的结果更多</li>
<li>设置合理的 <code>max_length_for_sort_data</code> 的值（一般不调整）</li>
<li>调小 <code>max_sort_length</code>，即排序时最多取多少字节</li>
</ul>
</li>
</ul>
<h4 id="group-by优化">GROUP BY优化</h4>
<p>MySQL有如下处理group by语句的方式，性能一次递减：</p>
<ul>
<li>松散索引扫描（Loose Index Scan）<br>
无需扫描满足条件的所有索引键即可返回结果。在explain语句的extra中会显示 <code>using index for group by</code>。但是sql需要满足：查询是作用在单张表上的，GROUP BY指定的所有字段要符合最左前缀原则，而且没有其他字段。如果存在聚合函数，只支持min()/max()，并且如果同时使用了min()和max()，则必须作用于同一个字段。聚合函数作用的字段必须在索引中，并且紧跟GROUP BY所指定的字段。如果查询中存在除GROUP BY指定的列以外的其他部分，则必须以常量的形式出现。</li>
<li>紧凑索引扫描（Tight Index Scan）</li>
<li>临时表（Temporary table）</li>
</ul>
<h3 id="表设计的优化">表设计的优化</h3>
<ol>
<li>
<p>表属性尽量不要设置为NULL。<br>
NULL是默认行为，如果不指定列字段为NOT NULL的，那么它就是NULL的。NULL并非不占用存储空间，反而需要MySQL需要额外的存储空间和逻辑处理，而且对于可为null的索引列，其使用和统计都会更加复杂。NULL参与的查询是需要使用IS NULL/IS NOT NULL作为条件的；当索引为NULL的值作为条件时，这个sql不会用到索引，NULL参与的计算，其结果都为NULL，聚合查询的结果都会忽略值NULL的记录；在排序中NULL值所在列也会排在最前面。</p>
</li>
<li>
<p>不随意设置数据类型<br>
创建表时要指定主键，主键不应该具有业务含义。虽然MySQL允许创建表时不指定主键，但是在表中没有非空整型唯一索引的时候InnoDB会自动添加隐式主键。<br>
选择合适的数据类型和恰当的取值范围。使用存储所需的最小数据类型；选择简单的数据类型；存储小数直接选择decimal；尽量避免使用text和blob。<br>
不建议使用枚举ENUM类型。枚举类型的值都是从允许值列表中选择的，这个列表在创建表结构就定义好了。枚举的优点是：枚举存储是数字，数据会更加紧凑；允许值提前定义，MySQL可以检查出数据的正确性。查询枚举列值的时候，默认显示的是枚举的值，如果想查询枚举的索引，需要给列加0。比如<code>select name, gender from user;</code> 可以查出枚举列gender的枚举值，而 <code>select name, gender + 0 from user;</code> 可以查出枚举列gender的枚举索引。使用枚举同样需要注意：</p>
<ul>
<li>对枚举列使用sum、avg等聚合函数的时候，会使用枚举列的索引。因为聚合函数的参数是数字；</li>
<li>对枚举列排序的时候，是根据其索引排序的；如果要按值排序，需要使用concat函数；</li>
<li>如果将数据存储到ENUM列中，则将数字视为可能值得索引，存储的值是具有该索引的枚举成员。如果存储的是字符串的数字，MySQL会先转成数值类型，去匹配索引序号。</li>
</ul>
</li>
<li>
<p>正确的使用索引。<br>
字符串类型的查询的时候没有使用引号，是不会使用索引的。where条件左边的字段参与了函数或数学运算，不会使用索引；联合索引最左前缀顺序不匹配，不使用索引。<br>
不再使用的索引需要及时删除，否则MySQL需要维护索引，浪费空间和性能。为选择性低的列创建索引意义不大。列值过长，可以选择部分前缀作为索引，而不是整列加上索引。表记录不超过1000行，不需要创建索引。一张表的索引不要超过5个。</p>
<blockquote>
<p>select count(distinct(concat(first_name, left(last_name, 3))))/count(*) from user;  # 查询出的值越接近1，代表选择性越好</p>
</blockquote>
</li>
</ol>
<h3 id="其他地方的优化">其他地方的优化</h3>
<ol>
<li>
<p>MySQL连接设置<br>
MySQL默认的连接超时是8小时(28800秒)，对应的配置参数是 <code>wait_timeout</code>。一般在连接的参数配置做自动重连。</p>
<blockquote>
<p>jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;chararchterEncoding=utf8&amp;autoReconnect=true</p>
</blockquote>
<p>但是autoReconnect是有副作用的。原有连接上的事务将会被回滚，连接中持有的表将会全部释放，连接关联的会话session将会丢失，连接中定义的用户变量将会丢失，连接中定义的预编译SQL将会丢失。可以修改MySQL配置，避免断开连接。</p>
<blockquote>
<p>[mysqld]<br>
wait_timeout = 288000<br>
interactive_timeout = 28800</p>
</blockquote>
<p>连接池的配置同样也会影响连接，如HikariCP配置：</p>
<ul>
<li>maximum-pool-size：最大连接数目。超过这个数目，新的数据库访问线程会被阻塞。推荐大小为cpu核心数*2+硬盘数；</li>
<li>minimum-idle：最小连接数目；</li>
<li>max-lifetime：最大连接生命时间，用来设置一个connection在连接池中的存活时间；</li>
<li>idle-timeout：一个链接idle状态的最大时长，超时则被释放。</li>
</ul>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shiro安全框架]]></title>
        <id>https://daffupman.github.io/a5-Z-yWdo/</id>
        <link href="https://daffupman.github.io/a5-Z-yWdo/">
        </link>
        <updated>2020-11-07T04:40:16.000Z</updated>
        <content type="html"><![CDATA[<p>最近的项目用了shiro框架，自己也搜索了一些入门资料。比较推荐的是《跟我学shiro》，内容翔实，举例丰富，有着配套的代码库。链接：https://www.iteye.com/blog/jinnianshilongnian-2018398。现在是对学习到的内容做个总结。</p>
]]></content>
    </entry>
</feed>